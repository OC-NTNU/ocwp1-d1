
\chapter{Text Mining}

\todo[inline]{Describe text mining, especially entity detection, relation extraction and event extraction in BioNLP}

There is a fast growing amount of \emph{structured data} in the form of very large databases, numerical data collections, multimedia data and log files, commonly referred to as \emph{Big Data} nowadays.
\emph{Data mining} -- also known as \emph{knowledge discovery} from data -- is about the discovery of interesting patterns in huge data collections through computational analysis \citep{han2006data}.
In addition, however, there is a parallel stream of so-called \emph{unstructured data} in the form of text.
This comprises traditional text sources such as newspapers, books, magazines and journals, as well as new media such webpages (Wikipedia, IMDB), blogs, social media (Facebook) and messaging (Twitter,SMS).
Much of the collective knowledge of the human race has historically been encoded in the form of text. 
Data mining methods can not handle text as input.
Therefore text needs to be converted to some form of structured data first in order to apply data mining techniques.
This is essentialy what \emph{text mining} is about: extracting structured information from text in order to discover patterns in huge text collections through automated processing.

Text mining is applied in many different areas and for many different purposes; for a recent overview see e.g. \citep{Aggarwal2012Mining,Weiss2012Fundamentals}.
Different domains require different approaches, techniques, tools and resources.
For example, opinion mining regarding movies from social media is quite different from trend detection in financial markets. 
In this report we focus on text mining of scientfic literature to support knowledge discovery, of which literature-based discovery is a particular variant.
Text mining on scientific literature is commonly regarded as a combination of four tasks: information retrieval, information extraction, natural language processing and data mining.

\emph{Information retrieval} (IR) addresses the task of searching a large document collection for documents that are relevent to a user's informational needs as expressed in a search query \citep{ManningRaghavanSchutze:08}. 
The most common IR applications nowadays are web search engines which try to retrieve relevant text documents from the web, mostly webpages, given a number of keywords or phrases.
Specialized search engines exists for particular documents collections, for instance, to search scientific publications in the area of biomedicine (PUBMED), because additional knowledge about the domain can be used to improve IR results.
IR is typically the first step in text mining in order to narrow down the set of relevant documents to a size managable for more in depth analysis.

\emph{Information extraction} (IE) is the process of automatically extracting structured information from text \citep{Jiang2012Information}.
What type of structured information is extracted depends on the application, but usually involves two general types of information: \emph{entities} and \emph{relations}.
Entities are the things mentioned in the text.
These can be general classes like people, organisations, companies or places, but can also be domain-specific such as proteins, genes, drugs, organisms or anatomical parts.
Finding entities in text is called \emph{named entity recognition}.
Relations hold among pairs of entities.
For example, a company can \emph{buy} another company, a certain protein may \emph{interact} with a certain gene, or a certain drug may \emph{cause} a certain side effect.
The task of finding predefined relations in text is known as \emph{relation extraction}.  
Relations may be more complex than simple binary relations between pairs of entities.
They may involve multiple entities playing different roles, with additional circumstances or conditions, or even relations between relations.
In this case, it is common to talk about \emph{events} and the task of \emph{event extraction}.

\emph{Natural language processing} (NLP) concerns automatic analysis of language by computers, usually from an applied point of view  \citep{jurafsky2000speech,manning1999foundations}.
Natural languages such as English or Chinese -- as opposed to artificial languages like traffic signs or programming languages -- are extremely complex and versatile symbolic systems that have evolved over time to enable humans to exchange information.
Language can convey many different types of information, ranging from simple facts to complex arguments or emotional states.
Notwithstanding tremendous progress in NLP over the last decades, computers are still far from really understanding language and currently manage a shallow grasp of meaning at best.
In practice, NLP for a specific domain, e.g. stock market reports or biomedical research articles, works significantly better than NLP for unrestricted input, because domain-specific language use and knowledge can be exploited to improve interpretation. 
Hence NLP techniques, tools and resources are often tied to a particular application domain.

Most IR systems rely to some extent on NLP.
Key word search, for instance, often involves a \emph{lemmatisation} step wich relates all morphological variants of a word to its stem.
For example, the verbs \emph{diving}, \emph{dives}, \emph{dove}, \emph{dived} are all linked to their base form \emph{dive}.
For many languages, this improves the recall of relevant documents at the (slight) expense of precision, i.e., some extra irrelevant documents are returned.
More advanced NLP techniques are keyword expansion with automatically learned synonymns and word sense disambiguation for keywords with more than one possible interpretation.  

IE tends to be highly dependent on NLP for linguistic analysis and interpretation of the input text prior to actual extraction.
Common NLP techniques are \emph{part-of-speech tagging} (labeling tokens according to their word class, such a noun, verb, preposition, etc.), lemmatisation (bringing back words to their lemma form) and syntactic parsing (analysis of the grammatical structure of sentences).
The resulting linguistic annotations of the text serve as input to the algorithms for entity recognition and relation extraction, which can range from hand-written pattern matching rules to data-driven machine learning algorithms that learn from examples..
Like domain-specific NLP, IE is usually rather restricted in scope.
That is, it targets a small subset of specific entities and relations of interest in the given domain, ignoring all other meaning contained in the text.    

Even with these limitations to a particular domain and to particular aspects of meaning, computers still make many mistakes in processing text, misinterpreting text and failing to detect important information.
Despite all their limitations, computers have one major advantage over humans when it comes to reading text: processing power.
Computers can process thousands or millions of documents in a relatively short time, something a human reader would never be able to match.
The main strenght of text mining thus comes from the ability to process big text data.
One of the challenges is how best to combine the fast but shallow and noisy processing capabilities of text mining systems with the slow but deep language understanding capibilities of humans.
This relates to fields such as human-computer interaction and information visualisation.   

\section{Text Mining in Biology and Medicine}
\label{sec:tm-biomed}

The first and most active area for research on, and application of, text mining is that of scientific publications in fields of biology and medicine, collectively referred to as \emph{biomedicine}.
One of the main reasons for this is the volume and rate of publications in biomedicine.
The MEDLINE (Medical Literature Analysis and Retrieval System Online) is a bibliographic database which indexes most biomedical publications from over 5,5000 journals since the 1950s.
Abstracts are searchable with PubMed, which currently contains over 23 million records and grows at a rate of approximately three publications a minute.
Full-text articles can be searched with PubMed Central and many alternative or specialized search engines are available. 
Even though these allow researchers to retrieve documents by means of keyword search, and refine search results in different ways, search queries may still return thousands of hits and checking all of them for relevant information is practically infeasible.
Text mining tools can offer a solution.
They can be be used for tasks such as further filtering of documents, clustering similar documents, automatically extracting information of interest, summarizing information through text and/or visualisation, and discovering regularities or inconsistencies.

There are already many excellent survey and review articles about text mining in biomedicine, including
\citep{Neves2012Survey,Simpson2012Biomedical,Andronis2011Literature,Ananiadou2010Event,RodriguezEsteban2009Biomedical,Zweigenbaum2009Advanced,Cohen2008Getting,Zweigenbaum2007Frontiers,Ananiadou2006,Erhardt2006Status,JenEA06,Spasic2005Text,Cohen2005Survey,Krauthammer2004Term,Blake2011Text,Krallinger2010Analysis}.
Since we do not want to duplicate these efforts with yet another survey, our discussion here is limited to a brief summary of those aspects related to information extraction and literature-based knowledge discovery.  

Generic text mining tools are not well-suited for biomedical text because of highly specialized language use (jargon). 
For example, many terms have a particular meaning or do not occur at all outside of biomedical texts.
Moreover, text mining is highly dependent on the type of application, because different goals require different methods and tools.
There are a couple of prototypical applications in biomedical text mining which have driven much of the research efforts and the development of tools and resources.
One instance is finding interactions among proteins in order to find biological pathways, which are series of actions among molecules giving rise to some product or a change in a cell.
Another instance is mining interations between drugs, or between a drugs and their side effects, with the goal of drug retargetting or repurposing, that is, finding new applications for existing drugs.

\subsection{Entity Detection}

\emph{Entity detection} -- often called named \emph{entity recognition} (NER) in other areas of NLP -- is the process of detecting mentions of entities in text.
In biomedicine, it is usually quite clear what the entities of interest are, for example, genes, proteins, cells, drugs, organisms, etc.
However, detecting entities is a non-trivial task due to a number of complications.

First, there is variation in spelling and naming of entity names.
For example, a gene name may be spelled as \emph{BRCA 2},\emph{ BRCA-2},\emph{Brca2} or \emph{Brca-2} \citep{Krallinger2010Analysis}.
The use of acronyms or abbreviations instead of full names is common. 
Typos are not uncommon.
All this variation in form means that it usually does not suffice to lookup a string in a given dictionary of named entities.

Second, there is ambiguitity: a term may mean different things depending on the context.
This is a common problem in NLP.
For instance, the word \emph{bat} may refer to flying animal or to a hitting instrument, depending on the context of use.
Automatically determining the right sense of a word is called \emph{word sense disambiguation}.
In biomedicine, this is often referred to as \emph{entity normalization}, where each entity mention is linked to a unique indentifier in a database, controlled vocabulary or ontology, so its meaning becomes unambiguous. 
 
Third, as science progresses in ceratin domain, so does the sublanguage to describe it, and consequently new terms are added all the time.
This means that systems for named entity detectection must be updated all the time. 
In addition, terms may gradually change meaning over time, a phenomenon known as \emph{semantic drift}.

\EM{tools, bionlp challenges}

\subsection{Relation Extraction}   

Relation extraction aims at automatic extraction of relations between entities from text.
These are usualy binary relations such as ``protein X activates gene Y'', ``drug X treats disease Y'' or ``gene X is associated with disease Y''.
It is usually assumed that entities are already detected prior to relation extraction and that the set of relations is very limited.
Still, relation extraction is a hard task because of the variation and ambiguity in language use.
There are so many different ways to express essentially the same relation in language, that listing all of them is practically impossible.
Hence straight forward lookup or template matching on words is bound to fail.
Most systems perform a linguistic analysis step first, resulting in a more abstract representation ranging from a normalized surface string to syntactic or semantic graph structures. 
Relations are then extracted by manually written pattern matching rules operating on the abstract representations.
Alternatively, and more popular nowadays, extraction is performed in a data-driven way, whwre supervised machine learning algorithms learn to recognize relations from traning instances labeled by human annotators.
Hybrid systems combine different approaches to obtain better performance.
 

event extraction


\section{Machine Reading and Open Information Extraction}

\todo[inline]{NLU, open IE}

\citet{Etzioni2011Search}

From DeepDive:
Knowledge-base construction (KBC) is the process of pop-
ulating a knowledge base (KB) with facts (or assertions)
extracted from text. It has recently received tremendous
interest from academia, e.g., CMU's NELL [2] and MPI's
YAGO [7,9], and from industry, e.g., IBM's DeepQA [5] and
Microsoft's EntityCube [16]. 


\todo[inline]{inference and reasoning, relation to GOFAI}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "ocwp1-d1"
%%% End: 