
\chapter{Introduction}

Scientific publications are sources of knowledge encoded in text.
Each one contributes to our collective knowledge and they are often considered the main end product of scientific activity.
Quantity and quality of publications, for instance measured through journal impact factors, are nowadays the primary performance indicator for individual researchers, research groups, research centres or universities.  
The ``publish or perish'' culture is one of the driving forces behind the ever growing amount of scientific publications.
Other factors include the ease of electronic publication as well as the many computational tools speeding up modern science, from search engines to DNA sequencing.
As a result, the scientific literature is growing at an astounding rate of four papers per minute on average.
In many disciplines, ths means has become almost impossible for individual researchers to keep track of all new developments reported in the literature, even in their relatively narrow fields of expertise.

Search engines alone, although an essential tool for bibliographic research, are not sufficient to cope with the massive amount of scientific literature available.
Many search queries result in thousands of hits and sifting through all these documents by hand to locate the relevant pieces of information is practically infeasible.
This is where techniques and applications from the field of natural language processing (NLP) may offer a better solution.
\emph{Question-Answering}, for example, attempts to directly find the correct answer in a large text collection, in resonse to question posed by the user.
\emph{Automatic summarization} aims at summarizing single texts, multiple texts about the same topic, at different levels of compression, optionally guided by specific points of interest as expressed by the user.
\emph{Information extraction} tries to extract structured information (tables) from text collection according to prespecified search patterns, e.g., ``organism A feeds on organism B''.
These are just three examples of common applications of NLP which can benefit researchers in coping with scientific literature.
None of these applications work flawlessly, but they have matured to a level that makes them practically useful tools in certain domains.
Moreover, ongoing research in NLP and related areas holds the promise for further improvement.     
  
The growth of science has also given rise to increasing levels of specialisation.
Where historically scientists and researchers would routinely read and study literature outside of their main fields of expertise, the time pressures of modern science make this harder and harder.  
The Renaissance idea of the ``homo universalis'' is beyond the reach of most mortal scientists.  
This is unfortunate, because the tendency of science to analyse and reduce its subject matter to smaller and smaller pieces ought to be counterbalanced by an effort to consolidate and synthesize existing knowledge in order to gain true insight.    
In other words, there is so much scientific knowledge that it is hard for us to see the big picture.

Part of the solution to this problem is likely to come from computational methods for efficient processing of text.
Such methods are developed in the field of \emph{text mining} -- also known as \emph{knowledge discovery from text} or \emph{text analytics} -- which combines methods from natural language processing, artificial intelligence and data mining.
The general goal of text mining is to discover interesting patterns in massive amounts of text by means of computers.
One popular branch nowadays is opion mining, which attempts to extract the general opinion about products, organisations or public figures from positive or negative statements expressed by individuals in webpages, blogs, twitter messages and other forms of electronic media.
When applied to scientific literature, the interesting patters targeted by text mining -- also known as \emph{literature mining} in this case -- are not opinions, but usually facts of some sort.
For instance, thousands of journals and books on paleobiology can be automtically processed to extract all instances of the fact that species X occurred during a certain geological time unit Y.
From these data, it becomes possible to draw a biodiversity curve, which represents variation in the overal number of species over the past million of years.
This illustrates how text mining may enable synthesis of existing knowledge burried in text, facilitating new insight on a larger scale.  

One particular variant of text mining is known as\emph{ literature-based knowledge discovery} (LBD), which originates from the earliest attempts at text mining by \citet{swa86a}.
LBD attempts to find hidden relations between concepts in order to suggest new hypotheses.
These relations usually span disjoint literatures of papers which do not cite each other. 
One of the prototypical examples is a finding in the literature that fish oils reduced blood viscosity, and another one that patients of Raynaud's disease tend to exhibit high blood viscosity. 
These two facts combined led to the new hypothesis that fish oils can be used in the treatment of Raynaud's disease, a hypothesis that was subsequently confirmed experimentally.

Text mining, like the NLP techniques it relies on, is far from perfect.
Language is a very complicated communication system and computers can obtain a shallow understanding of the meaning of a text at best.
In fact, computers make lots of errors, reading pieces of text the wrong way or failing to understand important aspects of text all together.
Claims to the extent that text mining is a magic bullet should be met with skepticism. 
Yet, computers have the ability to read through millions of publications and extract information from it.
Computers can check millions of extracted facts to see if they fit together or form statistically interesting patterns.
These are feats that are impossible for humans.
Even though computer processing of text is currently noisy and shallow, the gain comes from the large numbers.

The potential of text mining to facilitate consolidation and synthesis of existing knowledge, as well as to reduce the information overload of scientists, fits well within the Ocean-Certain framework.
The main topics addressed in the project -- food web and biological in relation to climate change -- are inherently multi-displinary, involving fields such as biology, chemistry, climatology, geography, geology and physics.
A central theme in the project is therefore consilience, that is, unification of knowledge across diciplines by promoting a mutual understanding.
Susbtantial efforts towards this goal are planned accordingly, cumulating in WP4.
The scientific literature poses a challenge for consilience, not only because of the volume of potentially relevant publications, but also because of disjoint research communities with different conventions and terminologies.
Text mining in WP1 therefore intends to find hidden relations between concepts, with the ultimate goal of suggesting new hypotheses.
The plan is to take existing work and port it to the domain of Ocean-Certain, reusing as much as possible existing theories, models, applications, tools and resources.
As a first step in that direction, this deliverable surveys text mining, with an emphasis on literature-based knowledge discovery.
The goal is to present the general framework and the state-of-the-art.
Since biomedine is the first, most popular and most advanced application area for text mining, a substantial part of the text is necessarily dedicated to biomedical text mining.
Text mining in climate, marine and environmental science turns out to be virtually non-existent.
However, efforts in related areas such as geology and ecology will be reviewed.


  

 
  



    

   
   
fits with the consilience of OC



%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "ocwp1-d1"
%%% End: 
