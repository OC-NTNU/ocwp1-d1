\chapter{Literature-based Knowledge Discovery}

\todo[inline]{key publications, approaches, software and resources on Literature-Based Knowledge Discovery}

\citet{swa86a} observed that if a literature $L_1$ asserted $a \to b$, and a disjoint literature $L_2$ asserted $b \to c$, then the concept denoted by $b$ could function as a bridge between $L_1$ and $L_2$, leading to the discovery of the hypothesis $a \to c$\footnote{A note on terminology: In the LBD literature, capital letters are normally used for the $A$, $B$ and $C$ concepts. In this report, minuscules will be used to represent individual concepts, while capital letters represent sets. 

Also, some authors use $A$ to denote the the goal concept, and $C$ for the starting concept. This report uses the most commonly used terminology, in which $a$ always denotes the starting concept, and $c$ denotes the goal concept.}. One example given by Swanson showed that fish oils reduced blood viscosity ($fish\;oil \to blood\;viscosity$), and that patients of Raynaud's disease tend to exhibit high blood viscosity ($blood\;viscosity \to Raynaud$). These two facts led to the hypothesis that fish oils can be used in the treatment of Raynaud's disease ($fish\;oil \to Raynaud$) when combined. This hypothesis was subsequently confirmed experimentally \cite{dig89}. Although the inference steps are not logically sound, the procedure is able to produce interesting results. The general approach of bridging disjoint literatures by means of intermediary terms has been dubbed \emph{Swanson linking}, and is also referred to as the \emph{ABC model}.

\citet{swa97} explain that the discovery of the ABC structure and the fish oil-Raynaud's disease connection happened accidentally. This discovery led Swanson to conduct literature searches aided by existing information retrieval tools to search for more undiscovered public knowledge using the ABC model, resulting in the discovery of eleven connections between migraine and magnesium \cite{swa88}. As the discovery process was extremely time consuming, requiring the researcher to read hundreds of papers, Swanson later developed a computational tool, Arrowsmith, to streamline the discovery process.

There are two modes of discovery in the ABC model: \emph{Open discovery} and \emph{closed discovery}. In open discovery, the researcher only knows the starting concept $a$, and is interested in uncovering undiscovered public knowledge related to $a$. A researcher who looks for consequences of ocean acidification might conduct an open discovery search with $a=ocean\ acidification$. In closed-discovery, the researcher knows both the starting concept $a$ and the goal concept $c$, and is interested in finding concepts $B$ that prove an explanation of the relationship between the two terms. A researcher who hypothesizes that ocean acidification might cause a reduction in phytoplankton population may try to discover the causality chain by conducting a closed discovery search with $a=ocean\ acidification,c=phytoplankton\ population$.

This section will present an overview of the state-of-the-art of the LBD field. As the focus of this report is LBD in the climate science domain, and most existing work on LBD has been conducted in the biomedical domain, approaches will be grouped into of three groups according to their dependence on domain specific tools and resources, because reliance on these is likely to hinder adaptation to other domains\footnote{Some of the papers are presented as domain independent, even though they employ domain specific resources, because their main research contributions can be adapted in a domain-independent manner.}.

\section{Group 1: Domain-independent approaches}

In the general Swanson linking paradigm, open discovery is conducted by extracting all relations $a \to b_i$ from the literature of $a$, written $L(a)$. For every $b_i$, all relations $b_i \to c_j$ are then extracted from $L(b_i)$. The set of all $a \to b_i \to c_j$ relations, dubbed \emph{discovery candidates} is then are presented to the user as potential discoveries, sorted according to some ranking metric.

In most LBD approaches $L(x)$ is defined as the set of documents returned when searching for $x$ in a literature database. The literature database most commonly used in LBD is Pubmed/Medline\footnote{\url{http://www.ncbi.nlm.nih.gov/pubmed/}}, maintained by the US National Library of Medicine. The original Arrowsmith system considered only paper titles, as Swanson considered these to hold the most compact knowledge, but it has become the standard approach in LBD to use abstracts and possibly index terms in addition to the titles. The motivation for this is that abstracts and index terms contain more knowledge than only titles. 

Somewhat surprisingly, few LBD systems use full paper texts.  \citet{sch04} show that 30-40\% of all information contained in a section is new to that section, meaning that significant amounts of knowledge is lost when only looking at abstracts and index terms of a paper. The need for full text data is also pointed out by \citet{cam13}. The reason for not using full text seems to be that paper abstracts and index terms are available in xml format through the Pubmed API, while full paper texts require accessing rights and are normally stored as pdf.

In co-occurrence based systems, a relation $x \to y$ is postulated if $x$ and $y$ exhibit a high degree of co-occurrence in $L(x)$, either in terms of absolute frequency of co-occurrence, or in terms of statistical unlikelihood given the statistical promiscuity of the two concepts. While a few systems use the sentence as the domain for counting co-occurrences, most systems count co-occurrences across entire abstracts.\PO{Reference?} 

To present the user with only potential new discoveries, most LBD systems remove from $C$ all terms that are already known to be in a relation with $a$. In co-occurrence based methods, this is done by removing any $(a,c)$ pairs that exhibit higher degrees if co-occurrence than a predefined threshold (normally 1 co-occurrence) in $L(a)$.

\subsection{Arrowsmith}
The original Arrowsmith system works as follows \cite{swa97}: $L(a)$ is fetched by conducting a Medline search to retrieve the titles of papers containing $a$ in the title. The set of potential B concepts is extracted as the list of unique words in $L(a)$, after a stop list of approximately 5000 words has been applied. The B-term set is further pruned by removing all the words that have lesser relative frequency in $L(a)$ than in Medline. The potential B terms are subsequently presented to the user, who can then remove words that are thought to be unsuitable. For each $b_i \in B$, $L(b_i)$ is retrieved and a set $C_i$ is generated, subject to the same stopword and frequency restrictions as before. The terms in the union of the $C_i$ sets are then ranked according to the number of $b$-terms that connect them to the $a$-term.

\subsection{Information retrieval-based methods}
\citet{gor96} \cite{lin99} developed a system in parallel, which differed from Arrowsmith in several ways: Firstly, while Arrowsmith was word-based, their system used n-grams as the unit of analysis. A stop list was applied by removing all n-grams that contained any stop word occurrence. Secondly, their system used entire Medline records, comprising of keywords, abstracts and titles, whereas Arrowsmith only used paper titles. Thirdly, their system employed information retrieval metrics such as \emph{tf*idf} to find $b$-terms among the generated candidates, whereas Arrowsmith was based on relative frequencies.

The lexical statistical approach is so generic that it lends itself directly to application in different domains. In a later paper, \citet{gor01} employ this approach to conduct LBD searches directly on the World Wide Web, searching for application areas for genetic algorithms. It should however be noted that the goal of this experiment was not LBD in the sense of uncovering undiscovered public knowledge, instead focusing in discovering something that might be ``publicly known'' but novel to the user.

\subsection{Ranking metrics}
\citet{wre04a} pointed out that the structure of concept co-occurrence relationships is such that most concepts are connected to any other concept within few steps. This \emph{small world phenomenon} implies that research focus should be shifted away from retrieving discovery candidates to ranking them, because a significant portion of the concept space will be retrieved even within two co-occurrence relation steps. The paper proposes ranking implicit relationships by comparing the number of observed indirect connections between $a$ and $c$ to the number of expected connections in a random network model, given the relative promiscuity of the intermediary terms. 

In another paper, \citet{wre04b} emphasizes the importance of using a statistically sound method of ranking relationship strengths, such as ``chi-square tests, log-likelihood ratios, z-scores or t-scores'', because co-occurrence based measures bias towards more general, and thus less interesting relationships. The paper further proposes an extension to the mutual information measure (MIM) as a ranking measure.

\subsection{Latent semantic indexing}
\citet{gor98} propose exploiting the ability of certain vector-based semantic models such as Latent semantic indexing (LSI) to discover implicit relationships between terms for LBD. They first train the semantic model on $L(a)$, and let the user choose as $b$ one of the terms most similar to $a$. A new semantic model is built from $L(b)$, and discovery candidates are ranked according to their similarity to $a$ in the $L(b)$-model. Their experiments showed that the resulting $b$- and $c$-term candidate lists closely resemble the lists produced by the information retrieval inspired lexical statistics. 

In another experiment they built a semantic model from a random sample of all of Medline, and looked directly for $c$-terms in the semantic model by considering the terms most similar to $a$.\PO{Same ref as Gordon \& Dumais?} 
This ``zoomed-out'' approach produced different results than the previous Swanson linking inspired approach, which the authors claimed meant that the two methods are complementary and could therefore be used in parallel, but no in-depth evaluation was conducted on the quality of the results. 

\section{Group 2: Concept-based approaches}

Several researchers advocate using domain specific concepts taken from an ontology or controlled vocabularies instead of n-gram tokens. Using concepts provides three benefits over n-gram models: Firstly, synonyms and spelling variants are mapped to the same semantic concept.  Secondly, using concepts allows for ranking and filtering according to semantic categories. Finally, it becomes easier to constrain the search space by removing spurious or irrelevant n-grams at an early stage, as they don't map to any concept in the domain. On the other hand, concept extraction from raw text is a non-trivial operation.

In LBD concept extraction is conducted in one of two ways: One option is to use NLP tools designed for entity recognition. The most commonly used in the biomedical domain is \emph{MetaMap} \cite{aro10}, which extracts concepts from the Unified Medical Language System (UMLS) meta-thesaurus\footnote{\url{http://www.nlm.nih.gov/research/umls/}}. The other option is to use Medical Subject Headings (MeSH)\footnote{\url{http://www.nlm.nih.gov/mesh/}}. MeSH is a controlled vocabulary for indexing biomedical papers, with which all Medline papers have been manually tagged. MeSH keywords can be queried directly from the Medline API. Both MeSH and UMLS terms are organized hierarchically according to semantic categories.

\subsection{DAD}

In their system, DAD (Disease-Adverse reaction-Drug), \citet{wee01} use MetaMap. They showed in an experiment that the number of concepts extracted is significantly lower than the number of n-grams, even after stop lists are applied (8,362 n-grams vs. 5,998 concepts). DAD also allows the user to specify which semantic categories to consider, by for instance only allowing concepts of the type \emph{pharmacological substance} as $c$ concepts, reducing the number of search paths significantly.

Their approach was able to replicate both Swanson's \emph{Raynaud's-fish oil} and \emph{migraine-magnesium} discoveries, but it was discovered that MetaMap maps both \emph{mg} (milligram) and \emph{Mg} (magnesium) to the concept \emph{magnesium}, giving optimistic results for the migraine-magnesium experiment. This is but one example showing that one of the problems with employing NLP tools in an LBD system is that system performance becomes closely tied to the performance of the tools it employs.

\subsection{LitLinker}

\citet{pra03} developed a system, LitLinker, which originally also used MetaMap, but they later found it too computationally expensive for practical use \cite{yet06}. MeSH terms are therefore employed instead.

In a preprocessing step, LitLinker calculates the co-occurrence patterns of every MeSH term across the literatures of every other MeSh term. For every MeSH term, the mean and standard deviation of co-occurrence counts across the literatures is calculated. In the discovery process, a term is considered to be related to another term if their co-occurrence is higher than statistically expected, based on its z-score.

Yetisgen-Yildiz and Pratt identified three classes of uninteresting links and terms that should be pruned automatically by system: (1) too broad terms (giving the examples \emph{medicine}, \emph{disease} and \emph{human}), (2) too closely related terms (giving the example \emph{migraine} and \emph{headache}), and (3) semantically nonsensical connections. The first class is handled by removing any concept if it is strictly more specific in the MeSH ontology hierarchy than any included term. The second class is handled by pruning all links between terms that are closely related (grandparents, parents, siblings and children) in the ontology. The third class is handled by letting the user specify which semantic classes of concepts are allowed to link.

\subsection{Bitola}

\citet{hri01} originally developed a system called Bitola\footnote{\url{http://ibmi3.mf.uni-lj.si/bitola/}} that discovered \emph{association rules} between MeSH terms. Association rules mining is a common data mining method for discovering relations between variables in a database. Association rules are traditionally used for market basket analysis, in which rules of the type $\{pizza,steak\}\to\{coca\;cola\}$ are inferred, stating that if somebody buys pizza and steak, he/she is likely to buy coca cola as well. In Bitola's discovery step, basic associations are first mined from the co-occurrence patterns of MeSH terms. Subsequently, indirect associations $a \to c$ are inferred by combining association rules on the form $a \to b_i$ and $b_i \to c$, and ranked according to the sum of strengths of the connecting association rules.

\section{Group 3: Relation extraction-based approaches}
\label{sec:relex}	

\citet{hri06} point out two problems with the co-occurrence based LBD systems: Firstly, no explicit explanation of the relation between the $a$ and $c$ terms is given. Secondly, a large number of spurious relations are discovered, as demonstrated by the low precision values witnessed during system evaluation. Both aspects increase the time needed to examine the output of the system by the human user. They suggest that employing natural language processing (NLP) techniques to extract explicit relations from the papers can improve performance on both points.

The biomedical information extraction tool most commonly used in LBD is \emph{SemRep} \cite{rin03}, which uses linguistically motived rules on top of the ouput from MetaMap and the Xerox POS Tagger to extract knowledge in the form of $<subject, predicate, object>$ relation triplets. Although the knowledge expressed in natural language is more complex than what can be represented in simple relation triplets, SemRep is able to provide a better approximation to the knowledge content of scientific papers than do co-occurrence based methods.

While most LBD research employs the same NLP tool, systems differ as to how the extracted relations are represented and how reasoning is conducted in the relation space. Some researchers closely follow the Swanson linking paradigm, and use relation extraction based method instead of or in addition to co-occurrence based methods for candidate generation and ranking. Other researchers take an approach motivated by Wren's observation that a small-world property holds in the network of concept relations in literature.\PO{Reference?}
As significant portions of the concept-relation space will have to be explored in a two-step search anyway, it might be better to extract all relations from the entire literature collection or from a random sample thereof, and rather focus on valid and efficient reasoning within the entire concept-relation space. 

\citet{sma12} critiques the usage of relation extraction in LBD and claims that while reasoning over explicit relations may lead to so-called \emph{incremental discoveries}, that is, discoveries that lie close to the existing knowledge and therefore are less interesting, they are not able to lead to any \emph{radical discoveries}, that is discoveries that seem unlikely at time of discovery. He also claims that human discoveries, both incremental and radical, tend to be on a higher level, using analogies and abstract similarities rather than explicit relations, and that the benefit from using relation extraction therefore is minimal\footnote{Smalheiser's critique also extends to many of the widely employed co-occurrence based methods. The argument is that research should focus on developing methods that rank interesting relations highly.}.

\subsection{Augmented Bitola}
\label{subsec:bitola}

In two papers, Hristovski et al. \citeyearpar{hri06,hri08} experiment with augmenting the Bitola system by using relation extraction tools. In addition to SemRep, they also use another tool, \emph{BioMedLee}, because each of the tools exhibits better performance than the other on certain types of relations. 

To guide search through the concept-relation space, they introduce the notion of a \emph{discovery pattern}. A discovery pattern is a set of concept types and relations between them that could imply an interesting relationship in the domain. One discovery pattern, \emph{maybe\_treats} can informally be stated as: If a disease leads to a biological change, and a drug leads to the opposite change, then the drug may be able to treat the disease.

The integration between Bitola and the NLP components presented in the system is rather crude; for a given query term, Bitola outputs a set of related terms and the set of papers connecting each related term to the query term. The connecting paper must then be manually input into the NLP components to extract the relation between the query term and any related term. Following a discovery pattern requires extracting relations between several concepts until a chain of the correct relations has been found. The possibility to integrate Bitola and the NLP tools more tightly has been raised as possible future work, but it has been noted a concern that the computational load increases as the NLP component becomes less constrained by the co-occurrence based components.

\subsection{Graph-based reasoning}

The extracted relations can be represented as a \emph{Predications Graph} in which each concept is represented by a node and each relation is a labelled, directed edge from the subject concept to the object concept. Representing the concept-relation space as a graph provides two benefits: As a visual tool, a graph can display the knowledge extracted by the system in a way that is easily understood by the user and can be navigated/explored easily. As a mathematical object, one can employ graph theoretic results when developing algorithms for the reasoning process. 

In the work of \citet{wil11} an initial graph is constructed by querying a pre-compiled database of predications extracted by SemRep from Medline for all relations containing the $a$ concept. The user then incrementally expands the graph by selecting which terms to query relations for from a list of concepts ranked by their degree centrality (i.e. their degree of connectivity in the graph). After graph construction, potential discovery paths are ranked according to summed degree centrality. 

Although some work has been conducted in graph-based LBD, seemingly no research has been conducted on LBD in a global, large-scale predications graph derived from all of Medline, or a sample of it.

\subsection{Predication-based semantic indexing}
\citet{coh12a} propose a hyperdimensional computing technique they call \emph{predication-based semantic indexing} (PSI) for efficient representation and reasoning in the concept-relation space. In PSI, concepts and relations are represented as high-dimensional vectors, where the semantic content of a concept's vector is a combination of all the relations it occurs in and all the concepts it is related to, weighted by the frequency of the relation. The system uses SemRep to extract relations from a sample of 8,182,882 Medline records as input to the training process. Inference in this hyperdimensional space can be performed by ordinary vector operations. The paper shows how PSI enables analogical reasoning along the lines of ``$x$ is to what as $y$ is to $z$?'' without explicitly traversing the intermediary relation paths between $y$ and $z$, leading to efficient inference. 

The system could originally only infer analogies along a single one of the pathways connecting two concepts $x$ and $y$. In a later paper \citet{coh12b} expanded the PSI to allow for analogies along multiple pathways, by introducing a vector operation simulating quantum superposition, efficiently reasoning over the entire subgraph connecting $x$ and $y$. The paper claims that because real world concepts tend to interact through several pathways, literature-based discovery should strive to be able to reason following a similar pattern.

\section{Evaluation metrics in LBD}
The first works of Swanson proved the validity of the LBD approach, as they were followed by clinical studies that confirmed the hypotheses generated by the LBD methodology. However, very little subsequent work has been examined that rigorously, possibly due the resources required to conduct a clinical study. 

The most common approach to evaluation taken by later systems, among others \cite{lin99,wee01,sri04}, has been to replicate one or more proven discoveries, often using Swanson's discoveries as a gold standard. In this approach, a system is considered successful if one of the C terms generated matches Swanson's hypothesis. 

Yetisgen-Yildiz and Pratt\cite{yet09} identify four aspects in which this evaluation methodology is insufficient:
\begin{itemize}
	\item The evaluation should consider the entire C term set. An evaluation based solely on the existence or lack thereof of a single term in the C term set is necessarily incomplete. A realistic evaluation should consider all discovery terms, spurious as well as correct.
	\item The evaluation methodology should be based on multiple experiments. A majority of researchers only tried to recreate one or two of Swanson's discoveries, leading to statistically unreliable results. For an evaluation to be representative of system performance, it must be repeatable for different starting terms.
	\item The evaluation methodology should be independent of any prior knowledge. When replicating a known discovery, it is easy to tune the system in a way that gives the desired results for the given problem, without improving the overall performance of the system. In other words, there is an overlap between the training data and test data, leading to a biased evaluation.
	\item The evaluation methodology should enable comparison of different systems. A methodology that only evaluates systems according to a binary classification (able or unable to replicate a discovery) cannot be used to quantitatively compare different approaches.
\end{itemize}

Instead, they propose an evaluation methodology that satisfies the aforementioned desiderata: They divide the data into two sets, the \emph{pre-cut-off set} which only consists of papers published before a given date, and the \emph{post-cut-off set} which consists of all the papers published after the cut-off date. They then conduct the discovery process on the pre-cut-off set, and calculate IR evaluation metrics by comparing the results to a gold standard based on the post-cut-off set. The gold standard is created by looking at any term co-occurrences in the post-cut-off set that do not occur in the pre-cut-off set, based on the assumption that a co-occurrence corresponds to a new discovery.

The basic IR evaluation metrics used are precision and recall, defined as $$P_i = \frac{\vert T_i \cap G_i \vert}{\vert T_i \vert}$$ $$R_i = \frac{\vert T_i \cap G_i \vert}{\vert G_i \vert}$$ where $T_i$ is the set of target terms generated by the system given starting term $i$, and $G_i$ is the set of terms in the gold standard created for starting term $i$. Several evaluation metrics can be derived from precision and recall, such as \emph{11-point average interpolated precision curves}, where the precision values are calculated for each of 11 recall levels (0 to 1 with increments of 0.1).

Using their evaluation methodology, Yetisgen-Yildiz and Pratt did the first quantitative evaluation of ranking and correlation-mining algorithms in LBD. The correlation-mining algorithms compared were tf-idf, z-score, mutual information measure, The ranking metrics that were evaluated were Linking term count (LTC), that is the number of $b$-terms connecting $a$ and $c$, Average minimum weight (AMW), that is the average weight of the $a \to b \to c$  connections, and Literature cohesiveness (COH), a measure developed by Swanson but not widely adopted. Experiments showed that LTC gave better precision at all levels of recall. The relation generation techniques that were considered were association rules, tf-idf, z-score and MIM. The experiment showed that association rules give the best precision score (8.8\%) but the worst recall score (53.76\%), while tf-idf gave the best recall (88.0\%) but a rather low precision (2.29\%). 

While the evaluation effort was an important contribution to the LBD field, the field could benefit from more quantitative evaluation is required. First of all, all candidate ranking/generation techniques and ranking metrics were tested with only one value of the parameters (for instance the cut-off thresholds for tf-idf and z-score). Comparing the performance of different settings for the parameters would yield a better understanding of each of the metrics, and could lead to results completely different than those reported. Secondly, only a small subset of possible relation generation/ranking techniques and discovery candidate ranking metrics were tested. For example, no relation extraction-based methods were included in the evaluation.

The authors identified two challenges with the proposed evaluation procedure: 
\begin{description}
\item[Selecting the cut-off date] Because the post-cut-off set should contain an approximation of all the possible discoveries from a starting term, it is important that it spans a sufficient amount of time, so actual discoveries have had time to emerge. On the other hand, it is important to place the cut-off date late enough for the pre-cut-off set to contain enough materials for realistic knowledge discovery.
\item[Creating the gold standard] Determining the gold standard discoveries from the post-cut-off set in non-trivial. Yetisgen-Yildiz and Pratt use co-occurrence, and define a new discovery as any two terms that co-occur in the post-cut-off set, and not in the pre-cut-off set, but this is clearly just an approximation.
\end{description}

A critique that can be raised against the evaluation procedure is that validity of the post-cut-off set as a gold standard is questionable for several reasons: \citet{kos07} pointed out that it is very difficult to verify that a discovery has not been made before the cut-off date. Also, as the mechanism for automatically creating the gold standard closely resembles the discovery mechanism, the sample is biased, and the evaluation is therefore bound to give optimistic estimates. Another question is whether quantitative measures reflect the usefulness of the LBD system: When at all is said and done, the usefulness of a LBD system equates to its ability to support user in discovering knowledge.



%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "ocwp1-d1"
%%% End: 